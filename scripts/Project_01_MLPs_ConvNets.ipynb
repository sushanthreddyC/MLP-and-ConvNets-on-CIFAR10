{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 763 Project 01: Training Multi-Layer Perceptrons and Convoluational Neural Networks on CIFAR10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives: \n",
    "\n",
    "- To get hands-on experiences on working with images (CIFAR10) and neural networks by implementing multi-layer perceptrons (MLPs) and convolutional neural networks (ConvNets).\n",
    "- To test different regularization strategies to get the best performance out of your models.\n",
    "- To investigate the limitations of using MLPs in this task, compared with ConvNets. \n",
    "- To investigate the potential risk of fitting noise of both MLPs and ConvNets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to Do?\n",
    "\n",
    "* Assume you have installed the ece763 environment. Run this notebook with that environment.\n",
    "* Write your code and text for all TODOs as shown by:\n",
    "\n",
    "```python\n",
    "    # TODO: ??? points - descriptions of what to do\n",
    "    raise NotImplementedError # comment it out after you write your code\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to submit your solutions\n",
    "\n",
    "* Add your NCSU ID as the postfix of the notebook filename, e.g., `Project_01_MLPs_ConvNets_twu19.ipynb`\n",
    "* Submit two versions of your notebook, one is fully executed with all outputs (`Project_01_MLPs_ConvNets_twu19_results.ipynb`), and the other with all outputs cleared (`Project_01_MLPs_ConvNets_twu19_empty.ipynb`). We will re-run the latter and expect the results will be exactly the same as those in the former.\n",
    "\n",
    "* Late policy:  5 free late days (counted using 0.5 unit, <=6 hours as 0.5 late day, otherwise 1 later day) in total â€“ use them in your ways; Afterwards, 25% off per day late; Not accepted after 3 late days per HW and Project. Not applicable to the final project. \n",
    "* Important Note: We will NOT accept any replacement of submission after deadline, even if you can show the time stamp of the replacement is earlier than the deadline. So, please double-check if you submit correct files.\n",
    "* Academic Integrity: Students are required to comply with the university policy on academic integrity found in the Code of Student Conduct found at http://policies.ncsu.edu/policy/pol-11-35-01 \n",
    "* Academic Honesty: See http://policies.ncsu.edu/policy/pol-11-35-01  for a detailed explanation of academic honesty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries to be Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import numpy as np \n",
    "import copy\n",
    "import time\n",
    "\n",
    "## Imports for plotting\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "## Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## typing\n",
    "from typing import (\n",
    "    Any,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    List,\n",
    "    Optional,\n",
    "    Sequence,\n",
    "    Set,\n",
    "    Tuple,\n",
    "    Type,\n",
    "    Union,\n",
    ")\n",
    "\n",
    "try:\n",
    "    from typing import Literal\n",
    "except ImportError:\n",
    "    from typing_extensions import Literal\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## PyTorch Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the system\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "from mmengine.utils.dl_utils import collect_env\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"=\" * 40, \"System Information\", \"=\" * 40)\n",
    "uname = platform.uname()\n",
    "print(f\"System: {uname.system}\")\n",
    "print(f\"Node Name: {uname.node}\")\n",
    "print(f\"Release: {uname.release}\")\n",
    "print(f\"Version: {uname.version}\")\n",
    "print(f\"Machine: {uname.machine}\")\n",
    "print(f\"Processor: {uname.processor}\")\n",
    "\n",
    "print(\"=\" * 40, \"Environment Information\", \"=\" * 40)\n",
    "my_env = collect_env()\n",
    "pprint(my_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds and Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for setting the seed\n",
    "def random_seed(seed: int = 42, rank: int = 0, deterministic: bool = False) -> None:\n",
    "    # TODO: 2 points  - write your code below\n",
    "    raise NotImplementedError # comment it out after you write your code\n",
    "\n",
    "seed = 42\n",
    "random_seed(seed=seed, deterministic=True)\n",
    "\n",
    "# Fetching the device that will be used throughout this notebook\n",
    "device = (\n",
    "    torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
    ")\n",
    "print(\"Using device\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets (CIFAR10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../checkpoints/ece763_proj_01\"\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10\n",
    "CIFAR10 is a very popular dataset for computer vision on low-resolution images (32x32 pixels). The task is to classify images into one of 10 classes: **airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics for normalizing the input values to zero mean and one std\n",
    "DATA_MEANS = [0.491, 0.482, 0.447]\n",
    "DATA_STD = [0.247, 0.243, 0.261]\n",
    "\n",
    "# Transformations are applied on images when we want to access them. Here, we push the images into a tensor\n",
    "# and normalize the values. However, you can use more transformations, like augmentations to prevent overfitting.\n",
    "# Feel free to experiment with augmentations here once you have a first running MLP, but remember to not apply\n",
    "# any augmentations on the test data!\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(),\n",
    "                                      transforms.Normalize(DATA_MEANS, DATA_STD)\n",
    "                                     ])\n",
    "\n",
    "# Loading the training dataset. We need to split it into a training and validation part\n",
    "main_dataset = CIFAR10(root=DATASET_PATH, train=True, transform=data_transforms, download=True)\n",
    "train_set, val_set = torch.utils.data.random_split(main_dataset, [45000, 5000], generator=torch.Generator().manual_seed(seed))\n",
    "\n",
    "# Loading the test set\n",
    "test_set = CIFAR10(root=DATASET_PATH, train=False, transform=data_transforms, download=True)\n",
    "\n",
    "# Create data loaders for later\n",
    "train_loader = data.DataLoader(train_set, batch_size=128, shuffle=True, drop_last=True, pin_memory=True, num_workers=3)\n",
    "val_loader = data.DataLoader(val_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)\n",
    "test_loader = data.DataLoader(test_set, batch_size=128, shuffle=False, drop_last=False, num_workers=3)\n",
    "\n",
    "# classes\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with data, it is always recommend to look at the data before blaming your model for not performing well if the data was incorrectly processed. Hence, let's plot 1-batch images of the CIFAR10 training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "end_time = time.time()\n",
    "print(f\"Time for loading a batch: {(end_time - start_time):6.5f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INV_DATA_MEANS = torch.tensor([-m for m in DATA_MEANS]).view(-1, 1, 1)\n",
    "INV_DATA_STD = torch.tensor([1.0 / s for s in DATA_STD]).view(-1, 1, 1)\n",
    "\n",
    "def imshow(img):\n",
    "    img = img.div_(INV_DATA_STD).sub_(INV_DATA_MEANS) # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(\"GroundTruth (1st row): \", \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are all set. So, let's dive into implementing our own MLP and ConvNet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Optimizer\n",
    "\n",
    "To gain a better insight in the training of the neural networks, let's implement our own optimizer. First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network's parameters given the gradients. Hence, we effectively implement a function $w^{t} = f(w^{t-1}, g^{t}, ...)$ with $w$ being the parameters, and $g^{t} = \\nabla_{w^{(t-1)}} \\mathcal{L}^{(t)}$ the gradients at time step $t$. A common, additional parameter to this function is the learning rate, here denoted by $\\eta$. Usually, the learning rate can be seen as the \"step size\" of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps. \n",
    "\n",
    "As most optimizers only differ in the implementation of $f$, we can define a template for an optimizer in PyTorch below. We take as input the parameters of a model and a learning rate. The function `zero_grad` sets the gradients of all parameters to zero, which we have to do before calling `loss.backward()`. Finally, the `step()` function tells the optimizer to update all weights based on their gradients. The template is setup below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizerTemplate:\n",
    "    \n",
    "    def __init__(self, params: nn.ParameterList, lr: float)->None:\n",
    "        self.params = list(params)\n",
    "        self.lr = lr\n",
    "        \n",
    "    def zero_grad(self)->None:\n",
    "        ## Set gradients of all parameters to zero\n",
    "        for p in self.params:\n",
    "            if p.grad is not None:\n",
    "                p.grad.detach_() # For second-order optimizers important\n",
    "                p.grad.zero_()\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def step(self)->None:\n",
    "        ## Apply update step to all parameters\n",
    "        for p in self.params:\n",
    "            if p.grad is None: # We skip parameters without any gradients\n",
    "                continue\n",
    "            self.update_param(p)\n",
    "            \n",
    "    def update_param(self, p: nn.Parameter)->None:\n",
    "        # To be implemented in optimizer-specific classes\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD) with momentum. Plain SGD updates the parameters using the following equation:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot g^{(t)}\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "The concept of momentum replaces the gradient in the update by an exponential average of all past gradients including the current one, which allows for a smoother training. The gradient update with momentum becomes:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n",
    "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot m^{(t)}\\\\\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "Let's implement the optimizer below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDMomentum(OptimizerTemplate):\n",
    "    \n",
    "    def __init__(self, params: nn.ParameterList, lr: float, momentum: float=0.9)->None:\n",
    "        super().__init__(params, lr)\n",
    "        self.momentum = momentum # Corresponds to beta_1 in the equation above\n",
    "        self.param_momentum = {p: torch.zeros_like(p.data) for p in self.params} # Dict to store m_t\n",
    "        \n",
    "    def update_param(self, p:nn.Parameter)->None:\n",
    "        # TODO: 10 points -- Implement the gradient update\n",
    "        raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify that our optimizer is working, let's create a challenging surface over two parameter dimensions which we want to optimize to find the optimum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pathological_curve_loss(w1: float, w2: float) -> float:\n",
    "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
    "    x1_loss = torch.tanh(w1)**2 + 0.01 * torch.abs(w1)\n",
    "    x2_loss = torch.sigmoid(w2)\n",
    "    return x1_loss + x2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curve(curve_fn, x_range: Tuple[float, float] = (-5,5), y_range: Tuple[float, float]=(-5,5), plot_3d: bool=False, cmap=cm.viridis, title: str=\"Pathological curvature\"):\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = fig.add_subplot(projection=\"3d\") if plot_3d else fig.add_subplot()\n",
    "\n",
    "    x = torch.arange(x_range[0], x_range[1], (x_range[1]-x_range[0])/100.)\n",
    "    y = torch.arange(y_range[0], y_range[1], (y_range[1]-y_range[0])/100.)\n",
    "    x, y = torch.meshgrid([x,y], indexing='ij')\n",
    "    z = curve_fn(x, y)\n",
    "    x, y, z = x.numpy(), y.numpy(), z.numpy()\n",
    "\n",
    "    if plot_3d:\n",
    "        ax.plot_surface(x, y, z, cmap=cmap, linewidth=1, color=\"#000\", antialiased=False)\n",
    "        ax.set_zlabel(\"loss\")\n",
    "    else:\n",
    "        ax.imshow(z.T[::-1], cmap=cmap, extent=(x_range[0], x_range[1], y_range[0], y_range[1]))\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(r\"$w_1$\")\n",
    "    ax.set_ylabel(r\"$w_2$\")\n",
    "    plt.tight_layout()\n",
    "    return ax\n",
    "\n",
    "sns.reset_orig()\n",
    "_ = plot_curve(pathological_curve_loss, plot_3d=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of optimization, you can image that $w_1$ and $w_2$ are weight parameters, and the curvature represents the loss surface over the space of $w_1$ and $w_2$. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.\n",
    "\n",
    "Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of $w_2$. However, if we encounter a point along the ridges, the gradient is much greater in $w_1$ than $w_2$, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.\n",
    "\n",
    "To test our algorithms, we can implement a simple function to train two parameters on such a surface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_curve(optimizer_func, curve_func=pathological_curve_loss, num_updates=100, init=[5,5]):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list\n",
    "        curve_func - Loss function (e.g. pathological curvature)\n",
    "        num_updates - Number of updates/steps to take when optimizing \n",
    "        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2\n",
    "    Outputs:\n",
    "        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.\n",
    "    \"\"\"\n",
    "    weights = nn.Parameter(torch.FloatTensor(init), requires_grad=True)\n",
    "    optimizer = optimizer_func([weights])\n",
    "    \n",
    "    list_points = []\n",
    "    for _ in range(num_updates):\n",
    "        # TODO: 8 points -- Determine the loss for the current weights, save the weights and loss, perform backpropagation\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        \n",
    "    points = torch.stack(list_points, dim=0).detach().cpu().numpy()\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's apply the optimizer on our curvature. Note that we set a much higher learning rate for the optimization algorithms as you would in a standard neural network. This is because we only have 2 parameters instead of tens of thousands or even millions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=10, momentum=0.9))\n",
    "print(SGDMom_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand best how the different algorithms worked, we visualize the update step as a line plot through the loss surface. We will stick with a 2D representation for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_points = SGDMom_points\n",
    "ax = plot_curve(pathological_curve_loss,\n",
    "                x_range=(-np.absolute(all_points[:,0]).max(), np.absolute(all_points[:,0]).max()),\n",
    "                y_range=(all_points[:,1].min(), all_points[:,1].max()),\n",
    "                plot_3d=False)\n",
    "ax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"red\", marker=\"o\", zorder=2, label=\"SGDMom\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the implementation is correct, you should see that the optimizer indeed reaches a point of very low $w_2$ ($w_2 < -7.5$) and $w_1\\approx 0$. If not, go back to your optimizer implementation and check what could go wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Generic training and validation function\n",
    "\n",
    "Now that we the optimizer implemented, and the dataset loaded, we can look at implementing our own training functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model: nn.Module, optimizer: OptimizerTemplate, loss_module, data_loader)->Tuple[float, int]:\n",
    "    true_preds, count = 0.0, 0\n",
    "    model.train()\n",
    "    for imgs, labels in data_loader:\n",
    "        # TODO: 10 points -- Implement training loop with training on classification\n",
    "        raise NotImplementedError\n",
    "       \n",
    "        # Record statistics during training\n",
    "        \n",
    "    train_acc = true_preds / count\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_model(model, data_loader):\n",
    "    # TODO: 10 points - Test model and return accuracy\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, model_name, root_dir=CHECKPOINT_PATH):\n",
    "    # TODO: 2 points -- Save the parameters of the model\n",
    "    raise NotImplementedError\n",
    "    \n",
    "\n",
    "def load_model(model, model_name, root_dir=CHECKPOINT_PATH):\n",
    "    # TODO: 2 points -- Load the parameters of the model\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_module, train_data_loader, val_data_loader, num_epochs=25, model_name=\"MyModel\"):\n",
    "    # Set model to train mode\n",
    "    model.to(device)\n",
    "    best_val_acc = -1.0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_acc = train_one_epoch(model, optimizer, loss_module, train_data_loader)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == num_epochs:\n",
    "            # Evaluate the model and save if best\n",
    "            acc = test_model(model, val_data_loader)\n",
    "            if acc > best_val_acc:\n",
    "                best_val_acc = acc \n",
    "                save_model(model, model_name, CHECKPOINT_PATH)\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {acc*100.0:05.2f}%, Best validation accuracy: {best_val_acc*100.0:05.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Load best model after training\n",
    "    model = load_model(model, model_name, CHECKPOINT_PATH)\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Implementing the MLP\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.1: Vanilla MLP Module\n",
    "\n",
    "You can make use of PyTorch's common functionalities, especially the `torch.nn` modules might be of help. The design choices of the MLP (e.g. the activation function) is left up to you, but for an initial setup, we recommend stacking linear layers with ReLU activation functions in between. Remember to not apply any activation function on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int=3072, num_classes: int=10, hidden_dims: List[int]=[256, 128], act_layer: nn.Module=nn.ReLU)->None:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimension of the input images in pixels\n",
    "            num_classes - Number of classes we want to predict. The output size of the MLP\n",
    "                          should be num_classes.\n",
    "            hidden_dims - A list of integers specifying the hidden layer dimensions in the MLP. \n",
    "                           The MLP should have len(hidden_sizes)+1 linear layers.\n",
    "            act_layer - Activation function.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # TODO: 5 points - Create the network based on the specified hidden sizes\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: 5 points -- Apply the MLP on an input\n",
    "        raise NotImplementedError\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters()) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the MLP implementation\n",
    "input_dim = np.random.randint(low=64, high=3072)\n",
    "num_classes = np.random.randint(low=5, high=20)\n",
    "hidden_dims = [np.random.randint(low=32, high=256) for _ in range(np.random.randint(low=1, high=3))]\n",
    "my_mlp = MLP(input_dim=input_dim, num_classes=num_classes, hidden_dims=hidden_dims)\n",
    "my_mlp.to(device)\n",
    "random_input = torch.randn(32, input_dim, device=device)\n",
    "random_output = my_mlp(random_input)\n",
    "assert random_output.shape[0] == random_input.shape[0]\n",
    "assert random_output.shape[1] == num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and start training\n",
    "model_mlp = MLP(act_layer=nn.ReLU).to(device)\n",
    "optimizer = SGDMomentum(model_mlp.parameters(), lr=0.1) # you may tune lr\n",
    "loss_module = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(f'model mlp created: {count_parameters(model_mlp):05.3f}M')\n",
    "model_mlp=train_model(model_mlp, optimizer, loss_module, train_loader, val_loader, num_epochs=5, model_name=\"myMLP_ReLU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best model on test set\n",
    "vanilla_mlp_test_acc = test_model(model_mlp, test_loader)\n",
    "print(f'Test accuracy: {vanilla_mlp_test_acc*100.0:05.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is expected that you reach with the default configurations a validation and test accuracy of $\\sim51-53\\%$ . If you have reached this performance, we can consider this task as completed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Improving the vanilla MLP\n",
    "\n",
    "Now that we have a basic MLP, let's try to improve over this default performance! Your task is to think about ways to maximize the performance of your MLP. Possible suggestions you can look at include:\n",
    " \n",
    "* Do data augmentations help the model to generalize?\n",
    "* Can regularization techniques (dropout, weight decay, etc.) help?\n",
    "* Do deeper models perform better? Or is it better to have wide networks?\n",
    "* Can normalization techniques (BatchNorm, LayerNorm, etc.) help?\n",
    "\n",
    "Your task is to improve your model to reach at least 56% on the test set! But can you get even above this? Consider this as a challenge! \n",
    "\n",
    "For this implementation, you can copy and then modify your codes above. List the changes that you have made and discuss what affect they have. Further, repeat the experiments for *at least 3 seeds* to report stable improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 10 points -- Improve the model and list the changes as code comments. \n",
    "# You may try different changes (e.g. those suggested above) individually or combine them. \n",
    "# You do not need to explore all the four suggestions above.\n",
    "# You can create as many code blocks as you need. \n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3: Investigating the limitations of MLP\n",
    "\n",
    "Now that we have seen how we can optimize our MLP, it is good to investigate the limitations of the model as well. Images have a natural grid structure where close-by pixels are strongly related. Does the MLP make use of this structure? To investigate this question, we will run two experiments:\n",
    " \n",
    "* Create a shuffle of pixels at the beginning of the training, and use the same shuffle throughout the training and validation.\n",
    "* At each training and validation step, sample a new shuffle of pixels.\n",
    " \n",
    " \n",
    "It is up to you whether you perform this investigation on the original plain MLP version or your optimized one. Implement a corresponding train and test function that support both variants of shuffling, and train two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 points - create a function that shuffles pixel values of images\n",
    "def shuffle_pixels(imgs: torch.Tensor, shuffle_idx_shared: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    B, C, H, W = imgs.shape\n",
    "    if shuffle_idx_shared is not None:\n",
    "        # shuffle the pixels using the provided shuffle idx\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        # Sample a shuffle idx and then shuffle the pixels\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualized the effects of pixel shuffle\n",
    "images_pixelshuffled = shuffle_pixels(images)\n",
    "print(images_pixelshuffled.shape)\n",
    "imshow(torchvision.utils.make_grid(images_pixelshuffled))\n",
    "print(\"GroundTruth (1st row): \", \" \".join(f\"{classes[labels[j]]:5s}\" for j in range(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training and test function that supports the shuffling of pixels, either fixed and shared among the entire dataset or newly generated for each batch\n",
    "\n",
    "def train_one_epoch_pixelshuffled(\n",
    "    model: nn.Module,\n",
    "    optimizer: OptimizerTemplate,\n",
    "    loss_module,\n",
    "    data_loader,\n",
    "    shuffle_idx_shared: Optional[torch.Tensor] = None,\n",
    ") -> Tuple[float, int]:\n",
    "    true_preds, count = 0.0, 0\n",
    "    model.train()\n",
    "    for imgs, labels in data_loader:\n",
    "        # TODO: 2 points - Implement training loop with training on classification\n",
    "        raise NotImplementedError\n",
    "        # Record statistics during training\n",
    "    train_acc = true_preds / count\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_model_pixelshuffled(\n",
    "    model: nn.Module,\n",
    "    data_loader,\n",
    "    shuffle_idx_shared: Optional[torch.Tensor] = None,\n",
    ") -> float:\n",
    "    # TODO: 2 points -- Test model and return accuracy\n",
    "    raise NotImplementedError\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_pixelshuffled(\n",
    "    model,\n",
    "    optimizer,\n",
    "    loss_module,\n",
    "    train_data_loader,\n",
    "    val_data_loader,\n",
    "    num_epochs=25,\n",
    "    model_name=\"MyMLP_pixelshuffled\",\n",
    "    shuffle_idx_shared: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    # Set model to train mode\n",
    "    model.to(device)\n",
    "    best_val_acc = -1.0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_acc = train_one_epoch_pixelshuffled(model, optimizer, loss_module, train_data_loader, shuffle_idx_shared=shuffle_idx_shared)\n",
    "\n",
    "        if epoch % 5 == 0 or epoch == num_epochs:\n",
    "            # Evaluate the model and save if best\n",
    "            acc = test_model_pixelshuffled(model, val_data_loader, shuffle_idx_shared=shuffle_idx_shared)\n",
    "            if acc > best_val_acc:\n",
    "                best_val_acc = acc\n",
    "                save_model(model, model_name, CHECKPOINT_PATH)\n",
    "\n",
    "            print(\n",
    "                f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {acc*100.0:05.2f}%, Best validation accuracy: {best_val_acc*100.0:05.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Load best model after training\n",
    "    model = load_model(model, model_name, CHECKPOINT_PATH)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and start training on fixed shuffling of pixels\n",
    "model_mlp_pixelshuffled_shared = MLP(act_layer=nn.ReLU).to(device)\n",
    "optimizer = SGDMomentum(model_mlp_pixelshuffled_shared.parameters(), lr=0.1)\n",
    "loss_module = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# TODO: 2 points -- Create a shared: shuffle_idx\n",
    "raise NotImplementedError\n",
    "\n",
    "model_mlp_pixelshuffled_shared = train_model_pixelshuffled(\n",
    "    model_mlp_pixelshuffled_shared,\n",
    "    optimizer,\n",
    "    loss_module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=5,\n",
    "    model_name=\"myMLP_ReLU_pixelshuffled_shared\",\n",
    "    shuffle_idx_shared=shuffle_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and start training on a new shuffling of pixels per batch/image\n",
    "model_mlp_pixelshuffled_batch = MLP(act_layer=nn.ReLU).to(device)\n",
    "optimizer = SGDMomentum(model_mlp_pixelshuffled_batch.parameters(), lr=0.1)\n",
    "loss_module = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "model_mlp_pixelshuffled_batch = train_model_pixelshuffled(\n",
    "    model_mlp_pixelshuffled_batch,\n",
    "    optimizer,\n",
    "    loss_module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=5,\n",
    "    model_name=\"myMLP_ReLU_pixelshuffled_batch\",\n",
    "    shuffle_idx_shared=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What results do you observe? What does this tell us about the MLP being aware of the image structure? Add your results and observations below.\n",
    "\n",
    "* TODO: 2 points -- write down your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Implementing the ConvNet\n",
    "\n",
    "We will repeat the experiments in Part 2, but with a ConvNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1: Vanilla ConvNet\n",
    "\n",
    "Let's start with a simple LeNet like ConvNet, consisting of a number of convolution blocks (e.g., two Conv+ReLU+MaxPool blocks) followed by a MLP consisting of two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, in_chans: int=3, base_dims: int=32, num_classes:int=10, act_layer=nn.ReLU)->None:\n",
    "        super().__init__()\n",
    "        # TODO: 2 points -- write code for two Conv+ReLU+MaxPool blocks\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "        self.mlp = MLP(input_dim=base_dims*2, num_classes=num_classes, hidden_dims=[base_dims*4, base_dims*2], act_layer=act_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.mean(dim=(2, 3))\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model, optimizer, and start training\n",
    "\n",
    "model_convnet = ConvNet(act_layer=nn.ReLU).to(device)\n",
    "optimizer = SGDMomentum(model_convnet.parameters(), lr=0.1)\n",
    "loss_module = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "print(f\"model convnet created: {count_parameters(model_convnet):05.3f}M\")\n",
    "model_convnet = train_model(\n",
    "    model_convnet,\n",
    "    optimizer,\n",
    "    loss_module,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=5,\n",
    "    model_name=\"myConvNet_ReLU\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test best model on test set\n",
    "vanilla_convnet_test_acc = test_model(model_convnet, test_loader)\n",
    "print(f\"Test accuracy: {vanilla_convnet_test_acc*100.0:05.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will observe accuracy around $\\sim41-43\\%$, worse than the vanilla MLP. Next, we will try to address the performance issue by modifying the specifications of ConvNets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.2: Improving the ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Part 2.2. Please explore how to redesign your ConvNet to improve the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 10 points -- Improve the model and list the changes as code comments.\n",
    "# You may try different changes (e.g. those suggested in Part 2.2) individually or combine them.\n",
    "# You do not need to explore all the four suggestions above.\n",
    "# You can create as many code blocks as you need.\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.3: Testing if ConvNets can address the limitations of MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Part 2.3. Test your improved ConvNet to compare the results with those in Part 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 points -- Train your best ConvNet using data with a shared pixel shuffle and check the accuracy for training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 points -- Train your best ConvNet using data with a # TODO: Train your best ConvNet using data with a per-batch pixel shuffle and check the accuracy for training, validation and testing datasets pixel shuffle and check the accuracy for training, validation and testing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fitting Noise\n",
    "\n",
    "We will re-create a training dataset with ground-truth labels shuffled (i.e., labels become noises). Then, we train our MLPs and ConvNets to check the training accuracy. If they could do a good job, it means that there are potential risks of fitting noises of deep neural networks, and we shall be careful in monitoring the data quality in supervised training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the Ground-Truth Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 2 points -- Create a training dataset with ground-truth labels shuffled\n",
    "raise NotImplementedError\n",
    "noise_train_dataset = \n",
    "\n",
    "# Create data loaders for later\n",
    "noise_train_loader = data.DataLoader(\n",
    "    noise_train_dataset,\n",
    "    batch_size=128,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    pin_memory=True,\n",
    "    num_workers=3,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify labels are shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_dataiter = iter(noise_train_loader)\n",
    "noise_images, noisey_labels = next(noise_dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(torchvision.utils.make_grid(noise_images))\n",
    "for i in range(0, len(noisey_labels), 8):\n",
    "    print(\n",
    "        f\"Noisy labels (row {i//8}): \",\n",
    "        \" \".join(f\"{classes[noisey_labels[i+j]]:5s}\" for j in range(8)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MLPs and ConvNets on the noisy datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5 points -- Train your best MLP and check the training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 5 points -- Train your best ConvNet and check the training accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You have reached the end of the practical, congratulations! Now, you should have a good idea of what it means to train a MLP, how we can use neural networks to perform image classification, what aspects there are to improve a networks performance, and what limitations and risks there are. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
